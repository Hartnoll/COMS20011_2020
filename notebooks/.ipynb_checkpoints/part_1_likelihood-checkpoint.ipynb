{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'widget'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3f72269d37b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib widget'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mipywidgets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFloatSlider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntSlider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteract\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteract_manual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBernoulli\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3-4.4.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3-4.4.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2077\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2079\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2080\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-104>\u001b[0m in \u001b[0;36mmatplotlib\u001b[0;34m(self, line)\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3-4.4.0/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3-4.4.0/lib/python3.6/site-packages/IPython/core/magics/pylab.py\u001b[0m in \u001b[0;36mmatplotlib\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Available matplotlib backends: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbackends_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_matplotlib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_matplotlib_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3-4.4.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36menable_matplotlib\u001b[0;34m(self, gui)\u001b[0m\n\u001b[1;32m   2935\u001b[0m         \"\"\"\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpylabtools\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m         \u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_gui_and_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpylab_gui_select\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgui\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3-4.4.0/lib/python3.6/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mfind_gui_and_backend\u001b[0;34m(gui, gui_select)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgui\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgui\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;31m# select backend based on requested gui\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackends\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;31m# We need to read the backend from the original data structure, *not*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'widget'"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "from ipywidgets import FloatSlider, IntSlider, interact, interact_manual\n",
    "from torch.distributions import Bernoulli\n",
    "from torch.distributions import MultivariateNormal as MvNormal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\bracket}[3]{\\left#1 #3 \\right#2}\n",
    "\\newcommand{\\b}{\\bracket{(}{)}}\n",
    "\\newcommand{\\Bernoulli}{{\\rm Bernoulli}\\b}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\X}{\\mathbf{X}}\n",
    "\\newcommand{\\m}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\P}{{\\rm P}\\b}\n",
    "\\newcommand{\\dd}[2][]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sh}{\\mathbf{\\hat{\\Sigma}}}\n",
    "\\newcommand{\\mh}{\\boldsymbol{\\hat{\\mu}}}\n",
    "\\newcommand{\\N}{\\mathcal{N}\\b}\n",
    "\\newcommand{\\det}{\\bracket{\\lvert}{\\rvert}}\n",
    "\\newcommand{\\sb}{\\bracket{[}{]}}\n",
    "\\newcommand{\\E}{\\mathbb{E}\\sb}\n",
    "\\newcommand{\\Var}{{\\rm Var}\\sb}\n",
    "\\newcommand{\\Cov}{{\\rm Cov}\\sb}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\newcommand{\\ph}{\\hat{p}}\n",
    "\\newcommand{\\at}{\\bracket{.}{\\rvert}}\n",
    "\\newcommand{\\w}{\\mathbf{w}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\Wh}{\\mathbf{\\hat{W}}}\n",
    "\\newcommand{\\Y}{\\mathbf{Y}}\n",
    "\\newcommand{\\L}{\\mathcal{L}}\n",
    "\\newcommand{\\wh}{\\mathbf{\\hat{w}}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\0}{\\mathbf{0}}\n",
    "\\newcommand{\\I}{\\mathbf{I}}\n",
    "$$\n",
    "\n",
    "<h1> Part 1: Likelihoods and modelling </h1>\n",
    "\n",
    "First things first:\n",
    "<ul>\n",
    "  <li> I'm experimenting with doing the lectures in Jupyter Notebooks.</li>\n",
    "  <li> Hopefully, it means we can connect theory and code more closely.</li>\n",
    "  <li> As much as possible, I'll try to prepare the ground for later deep-leaning and machine learning courses.</li>\n",
    "  <li> I'm going to use PyTorch, mainly for the distributions library.</li>\n",
    "  <li> PyTorch's syntax is almost exactly the same as Numpy.</li>\n",
    "  <li> And so you can't copy code for labs/courseworks (which must be done in Numpy).</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> What is a model? </h2>\n",
    "\n",
    "Model == a structured approach for making predictions about the future.\n",
    "\n",
    "For instance, imagine Alice tosses a coin 10 times and gets:\n",
    "\n",
    "H T H T H T H T H H\n",
    "\n",
    "Now she tosses the coin again.  What happens?\n",
    "\n",
    "One option is that the coin tosses are indeed fair and random, in which case next time we might get:\n",
    "\n",
    "T H H H H T H T T T\n",
    "\n",
    "or the coin might be biased, and we just happened to get an equal number of heads/tails.  Next time, we might get:\n",
    "\n",
    "T H H H H T H H H H\n",
    "\n",
    "or Alice might have clever trick (https://www.youtube.com/watch?v=A-L7KOjyDrE), so that next time she can get exactly the same thing:\n",
    "\n",
    "H T H T H T H T H H\n",
    "\n",
    "(or whatever else she wants).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Simple example: A biased coin flip</h2>\n",
    "$$\n",
    "\\newcommand{\\bracket}[3]{\\left#1 #3 \\right#2}\n",
    "\\newcommand{\\b}{\\bracket{(}{)}}\n",
    "\\newcommand{\\Bernoulli}{{\\rm Bernoulli}\\b}\n",
    "$$\n",
    "Lets assume that Alice doesn't have a trick, so the coin tosses are independent and random, but that the coin might be biased.\n",
    "In that case, the coin-toss, $x$ can be said to be Bernoulli distributed, with probability $p$,\n",
    "\n",
    "$$P(x| p) = \\Bernoulli{x; p}$$\n",
    "\n",
    "where,\n",
    "\n",
    "$$\\Bernoulli{x; p} = \\begin{cases} 1-p &\\text{ if } x=0 \\\\ p &\\text{ if } x=1 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bern = Bernoulli(probs=0.7)\n",
    "print(bern.log_prob(0.).exp())\n",
    "print(bern.log_prob(1.).exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: PyTorch only provides the log_prob method, because this is much more numerically stable when doing deep-learning.  We therefore have to exponentiate to get back the probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, given a probability, $p$, we can make a predictions about possible values of about Alice's future coin tosses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_bernoulli(p):\n",
    "    return Bernoulli(probs=p).sample((10,)).to(dtype=t.int)\n",
    "interact_manual(sample_bernoulli, p=FloatSlider(value=0.5, min=0, max=1, step=0.05));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key question: how do we find p?\n",
    "\n",
    "<h3> Maximum likelihood fitting of a Bernoulli </h3>\n",
    "\n",
    "Consider data:\n",
    "\n",
    "T T T T H T T T T T T H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs = t.tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood, i.e. the probability of all the data, treated as a function of the parameter, $p$, is\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\x| p} = \\prod_\\lambda \\P{x_\\lambda| p}\n",
    "\\end{align}\n",
    "\n",
    "But these numbers rapidly shrink (and become too small for standard floating points).  Instead, we work with the log-likelihood,\n",
    "\n",
    "\\begin{align}\n",
    "  \\L(p) = \\sum_\\lambda \\log \\P{x_\\lambda| p}\n",
    "\\end{align}\n",
    "\n",
    "Importantly, the log-likelihood is considered a function of the parameter(s), $p$, but not the data, which is considered fixed.\n",
    "\n",
    "The goal is to find $\\ph$ which maximizes $\\L(p)$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\ph = \\argmax_{p} \\L(p)\n",
    "\\end{align}\n",
    "\n",
    "Lets find $ \\ph $!  We can code-up the log-likelihood using,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(p):\n",
    "    return Bernoulli(probs=p).log_prob(xs).sum(-1, keepdim=True)\n",
    "\n",
    "interact(log_likelihood, p=FloatSlider(min=0.01, max=0.99, step=0.01));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the log-likelihood,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = t.linspace(0.01, 0.99, 99)[:, None]\n",
    "print(ps.shape)\n",
    "ps[:6, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lls = log_likelihood(ps)\n",
    "lls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(ps, lls)\n",
    "ax.set_xlabel(\"p\")\n",
    "ax.set_ylabel(\"log-likelihood\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In simple cases like this, we could just use the above plot to find the value of $ p $ with the maximal log-likelihood.\n",
    "\n",
    "But in more complicated cases with many more parameters, this is no longer possible.\n",
    "\n",
    "Instead, the first approach is to try to find an analytic expression for the $p$ with the maximal log-probability.\n",
    "\n",
    "Remember,\n",
    "\n",
    "\\begin{align}\n",
    "  \\L(p) &= \\sum_\\lambda \\log \\P{x_\\lambda| p}\\\\\n",
    "  \\L(p) &= \\b{\\sum_\\lambda x_\\lambda} \\log p + \\b{N - \\sum_\\lambda x_\\lambda} \\log (1-p)\n",
    "\\end{align}\n",
    "\n",
    "And solve for where the gradient is zero,\n",
    "\n",
    "\\begin{align}\n",
    "  0 &= \\at{\\dd[\\L(p)]{p}}_{p=\\ph}\\\\\n",
    "  0 &= \\frac{\\sum_\\lambda x_\\lambda}{\\ph} - \\frac{N - \\sum_\\lambda x_\\lambda}{1-\\ph}\\\\\n",
    "  \\frac{\\sum_\\lambda x_\\lambda}{\\ph} &= \\frac{N - \\sum_\\lambda x_\\lambda}{1-\\ph}\\\\\n",
    "  (1-\\ph) \\sum_\\lambda x_\\lambda &= \\ph \\b{N - \\sum_\\lambda x_\\lambda}\\\\\n",
    "  \\sum_\\lambda x_\\lambda &= \\ph N\\\\\n",
    "  \\ph &= \\tfrac{1}{N} \\sum_i x_i\n",
    "\\end{align}\n",
    "\n",
    "The maximum-likelihood probability is the empirical mean of the data points.  This is sensible, but not at all obvious: we needed to go through the derivation!\n",
    "\n",
    "Using this derivation, we can write a function that automatically fits a Bernoulli distribution,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_bernoulli(xs):\n",
    "    p = xs.sum() / xs.shape[-1]\n",
    "    return Bernoulli(probs=p)\n",
    "\n",
    "fitted_bernoulli = fit_bernoulli(xs)\n",
    "fitted_bernoulli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "such that samples from the fitted Bernoulli look like the data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_bernoulli.sample((12,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Bayesian fitting of a Bernoulli </h3>\n",
    "\n",
    "Maximum likelihood works well when we have a reasonable number of datapoints.\n",
    "\n",
    "But what about when we have very little data (e.g. we might have many biased coins, and be able to toss each one a few times).\n",
    "\n",
    "In particular, lets say we have a population of coins, whose probabilities are drawn from a uniform distribution,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{p} &= \\text{Uniform}(p; 0, 1)\n",
    "\\end{align}\n",
    "\n",
    "we take out one coin, toss it twice, and get two zeros,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs = t.tensor([0., 0.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we say about $p$?\n",
    "\n",
    "Maximum likelihood would tell us that $ \\ph = 0 $,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_bernoulli(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is a bit strange, because $ p $ could have been quite large, but coincidentally, our two coin-tosses happened to be zeros.\n",
    "\n",
    "To understand this in a bit more depth, we can run a simulation.\n",
    "\n",
    "We start by drawing a large number of $ p $'s, from the uniform prior,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10**5\n",
    "ps = t.rand(N)\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"p\")\n",
    "ax.set_ylabel(\"P(p)\")\n",
    "ax.hist(ps, density=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the values on the x-axis are probability *densities*, not probabilities.\n",
    "\n",
    "The probability of being in any given bin is:\n",
    "\\begin{align}\n",
    "  \\int_{p_0}^{p_0 + \\delta} dp \\; \\P{p} \\approx \\delta \\P{p}\n",
    "\\end{align}\n",
    "i.e. the bin-width times the probability density.\n",
    "\n",
    "Here, we have $10$ bins, with probability density $1$, and bin width $\\delta = 1/20$.\n",
    "\n",
    "The probability of being in any 1 bin is therefore $\\P{p} \\times \\delta = 1 \\times 1/10 = 1/10$, and adding up the $10$ bins gives us $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for each of these $ p $'s, we toss a couple of coins,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = Bernoulli(ps).sample((2,))\n",
    "xs\n",
    "xs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we filter out only those values of $ p $ which actually gave us two zeros,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_zeros = (xs==0.).all(0)\n",
    "print(all_zeros)\n",
    "print(all_zeros.sum())\n",
    "ps_all_zeros = ps[all_zeros]\n",
    "print(ps_all_zeros.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot those $ p $'s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"p\")\n",
    "ax.set_ylabel(\"P(p|x=[0,0])\")\n",
    "ax.hist(ps_all_zeros, density=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense: we expect the probability to be small, because when we flipped the coin, we observed two zeros.\n",
    "\n",
    "But the probability could still be large, with the two zeros we observed being coincidences.\n",
    "\n",
    "It turns out this idea (sampling until we get something very close to the data, then looking at the corresponding latents, here the probability, $p$), is a real algorithm, called \"approximate Bayesian computation\" (ABC).\n",
    "\n",
    "But its a terrible idea: don't do it in practice unless you really have to.  For any non-trivial dataset, you will be waiting a very, very long time before random sampling produces something \"sufficient close\".\n",
    "\n",
    "Instead, can we do exact Bayesian inference to directly compute the distribution $\\P{p| \\x}$.\n",
    "\n",
    "The answer is yes!\n",
    "\n",
    "In particular, the law of joint probability tells us that we can write the joint, $\\P{\\x, p}$ in two equivalent forms,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\x, p} &= \\P{\\x| p} \\P{p} = \\P{p| \\x} \\P{\\x}\n",
    "\\end{align}\n",
    "\n",
    "The first form $\\P{\\x| p} \\P{p}$, is the standard one, and we can readily compute it given the expressions above.  The second form, $\\P{p| \\x} \\P{\\x}$ is a bit more unusual: it isn't immediately obvious how we can compute these terms.\n",
    "\n",
    "Nonetheless, we can rearrange to compute the term we're interested in,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{p| \\x} &= \\frac{\\P{\\x| p} \\P{p}}{\\P{\\x}} \\propto \\P{\\x| p} \\P{p}\\\\\n",
    "  \\log \\P{p| \\x} &= \\L\\b{p} + \\log \\P{p} + \\text{const}\n",
    "\\end{align}\n",
    "\n",
    "The proportionality arises because we take data, $\\x$, to be fixed, so we only care about parameter, $p$ dependence (as long as we make sure the distribution normalizes!\n",
    "\n",
    "Lets do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = t.Tensor([0., 0.])\n",
    "\n",
    "def log_prior(p):\n",
    "    return 0.\n",
    "\n",
    "def log_likelihood(p):\n",
    "    return Bernoulli(p).log_prob(xs).sum(-1, keepdim=True)\n",
    "\n",
    "ps = t.linspace(0.005, 0.995, 100).view(-1, 1)\n",
    "dp = ps[1] - ps[0]\n",
    "    \n",
    "unnorm_log_posterior = log_prior(ps) + log_likelihood(ps)\n",
    "unnorm_posterior = unnorm_log_posterior.exp()\n",
    "norm_posterior = unnorm_posterior / (dp * unnorm_posterior.sum())\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"p\")\n",
    "ax.set_ylabel(\"P(p|x=[0,0])\")\n",
    "ax.hist(ps_all_zeros, density=True);\n",
    "ax.plot(ps, norm_posterior);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to analytically compute the distribution over $p$ for a coin-toss.  But this doesn't give us much additional insight.  Instead, we'll do this below for a Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>More interesting examples</h2>\n",
    "In the previous example, a datapoint was either 0 or 1,\n",
    "\n",
    "```\n",
    "X = {0, 1}\n",
    "```\n",
    "\n",
    "but $ x $ could take on almost any other type.  Common examples include:\n",
    "\n",
    "```\n",
    "X = Vector{Float}   # A vector\n",
    "X = Str             # A string\n",
    "X = Image           # An image\n",
    "```\n",
    "\n",
    "Complex, state-of-the-art models for images (GANs) and text (GTP-2) do exactly what we described above.\n",
    "Then, they take a large dataset of images/text do a very large amount of processing, to fit a distribution to that data.\n",
    "Once the distribution has been fitted, they can draw samples from that distribution, that should look like the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Multivariate Normal (Gaussian)</h2>\n",
    "The multivariate Normal is the most important distribution over vectors.\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\x| \\m, \\S} &= \\N{\\x; \\m, \\S}\\\\\n",
    "  \\log \\N{\\x; \\m, \\S} &= -\\tfrac{1}{2} \\log \\det{2 \\pi \\S} - \\tfrac{1}{2} \\b{\\x - \\m} \\S^{-1} \\b{\\x-\\m}^T\n",
    "\\end{align}\n",
    "\n",
    "The expectation of the distribution is given by $\\m$ and the covariance is given by $\\S$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\E{\\x} &= \\m\\\\\n",
    "  \\Cov{\\x} &= \\S\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get some intuition for what the covariance means, we can plot samples from a multivariate normal with the same variance for $x_0$ and $x_1$, but different covariances,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(c):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"$x_0$\")\n",
    "    ax.set_ylabel(\"$x_1$\")\n",
    "    ax.set_xlim(-5, 5)\n",
    "    ax.set_ylim(-5, 5)\n",
    "    \n",
    "    mu = t.zeros(2)\n",
    "    Sigma = c*t.ones(2, 2) + (1-c)*t.eye(2)\n",
    "    print(\"Sigma =\")\n",
    "    print(Sigma)\n",
    "    dist = MvNormal(mu, Sigma)\n",
    "    xs = dist.sample((10000,))\n",
    "    ax.scatter(xs[:, 0], xs[:, 1])\n",
    "    \n",
    "interact_manual(plot, c=FloatSlider(value=0, min=-0.99, max=0.99, step=0.01));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, consider positive $\\Cov{x_0, x_1} = \\E{x_0 x_1}$.  To get positive covariances, $x_0$ and $x_1$ will tend to have the same sign (either both positive or both negative), so the overall product, $x_0 x_1$ is usually positive.\n",
    "\n",
    "Now, consider negative $\\Cov{x_0, x_1} = \\E{x_0 x_1}$.  To get negative covariances, $x_0$ and $x_1$ will tend to have the different signs (one positive and the other negative), so the overall product, $x_0 x_1$ is usually negative.\n",
    "\n",
    "Now, consider $0 = \\Cov{x_0, x_1} = \\E{x_0 x_1}$.  In this case, $x_0$ and $x_1$ are unrelated.\n",
    "\n",
    "As the covariance approaches the variance, the distribution gets narrower.\n",
    "\n",
    "Until eventually, the only way of achieving,\n",
    "\n",
    "\\begin{align}\n",
    "  1 = \\Var{x_0} = \\Var{x_1} \\approx \\Cov{x_0, x_1} = \\E{x_0 x_1}\n",
    "\\end{align}\n",
    "\n",
    "is by setting,\n",
    "\n",
    "\\begin{align}\n",
    "  x_0 = x_1\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Maximum likelihood fitting</h3>\n",
    "\n",
    "Maximum likelihood parameters, $\\m$ and $\\S$, for a multivariate normal is given by the empirical mean, $\\mh$ and covariance $\\Sh$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\mh, \\Sh = \\argmax_{\\m, \\S} \\sb{\\sum_\\lambda \\log \\N{\\x_\\lambda| \\m, \\S}}\n",
    "\\end{align}\n",
    "\n",
    "This seems sensible, but requires some fairly hairy matrix differentials to prove."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
